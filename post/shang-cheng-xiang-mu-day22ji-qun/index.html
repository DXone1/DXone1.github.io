<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>商城项目 day22：集群 | Gridea</title>
<link rel="shortcut icon" href="https://dxone1.github.io//favicon.ico?v=1639297828823">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://dxone1.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="商城项目 day22：集群 | Gridea - Atom Feed" href="https://dxone1.github.io//atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">



    <meta name="description" content="MySQL集群
MySQL-MMM 是 Master-Master Replication Manager for MySQL（mysql 主主复制管理器） 的简称， 是 Google 的开源项目（Perl 脚本） 。 MMM 基于 MyS..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://dxone1.github.io/">
  <img class="avatar" src="https://dxone1.github.io//images/avatar.png?v=1639297828823" alt="">
  </a>
  <h1 class="site-title">
    Gridea
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              商城项目 day22：集群
            </h2>
            <div class="post-info">
              <span>
                2021-12-11
              </span>
              <span>
                13 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h2 id="mysql集群">MySQL集群</h2>
<p>MySQL-MMM 是 Master-Master Replication Manager for MySQL（mysql 主主复制管理器） 的简称， 是 Google 的开源项目（Perl 脚本） 。 MMM 基于 MySQL Replication 做的扩展架构， 主要用来监控 mysql 主主复制并做失败转移。 其原理是将真实数据库节点的IP（RIP） 映射为虚拟 IP（VIP） 集。mysql-mmm 的监管端会提供多个虚拟 IP（VIP） ， 包括一个可写 VIP，多个可读 VIP， 通过监管的管理， 这些 IP 会绑定在可用 mysql 之上， 当某一台 mysql 宕机时， 监管会将 VIP迁移至其他 mysql。 在整个监管过程中， 需要在 mysql 中添加相关授权用户， 以便让 mysql 可以支持监理机的维护。<br>
<img src="https://dxone1.github.io//post-images/1639213733205.png" alt="" loading="lazy"><br>
InnoDB Cluster 支持自动 Failover、 强一致性、 读写分离、 读库高可用、 读请求负载均衡， 横向扩展的特性， 是比较完备的一套方案。 但是部署起来复杂， 想要解决 router单点问题好需要新增组件， 如没有其他更好的方案可考虑该方案。 InnoDB Cluster 主要由 MySQL Shell、 MySQL Router 和 MySQL 服务器集群组成， 三者协同工作， 共同为MySQL 提供完整的高可用性解决方案。 MySQL Shell 对管理人员提供管理接口， 可以很方便的对集群进行配置和管理,MySQL Router 可以根据部署的集群状况自动的初始化， 是客户端连接实例。 如果有节点 down 机， 集群会自动更新配置。 集群包含单点写入和多点写入两种模式。 在单主模式下， 如果主节点 down 掉， 从节点自动替换上来，MySQL Router 会自动探测， 并将客户端连接到新节点。<br>
<img src="https://dxone1.github.io//post-images/1639213936294.png" alt="" loading="lazy"></p>
<h3 id="mysql企业级部署">Mysql企业级部署</h3>
<p><img src="https://dxone1.github.io//post-images/1639214175874.png" alt="" loading="lazy"><br>
在大数据量情况下需要做分库分表，提升Mysql的检索效率。通过在主库开启日志并给一个账号权限，从库使用这个可读的权限即可同步主库的数据。</p>
<h3 id="shardingsphere">ShardingSphere</h3>
<p><img src="https://dxone1.github.io//post-images/1639217771624.png" alt="" loading="lazy"><br>
ShardingProxy可以执行分库分表策略，按照官网介绍配置主从和分库分表的策略，则以后连接ShardingProxy而不是直接数据库的端口。ShardingProxy启动以后如下图<br>
<img src="https://dxone1.github.io//post-images/1639227443247.png" alt="" loading="lazy"><br>
在sqlyog建立新的连接，直接连接到localhost:3388（因为是在windows上使用<code>start.bat 3388</code>启动）<br>
在配置中指定了分表在sharding_db中暴露，这样在这里面进行的更新操作都会分库分表到按策略计算的实际数据库表中<br>
<img src="https://dxone1.github.io//post-images/1639228331290.png" alt="" loading="lazy"><br>
在其中创建两个表<br>
<img src="https://dxone1.github.io//post-images/1639228552019.png" alt="" loading="lazy"><br>
实际数据库中也会存在<br>
<img src="https://dxone1.github.io//post-images/1639228599995.png" alt="" loading="lazy"><br>
执行如下的sql语句，从ShardingProxy看来是都存在了一个表t_order中<br>
<img src="https://dxone1.github.io//post-images/1639228944181.png" alt="" loading="lazy"><br>
但是实际的存储中则分库分表了<br>
<img src="https://dxone1.github.io//post-images/1639229097611.png" alt="" loading="lazy"></p>
<h2 id="redis集群">Redis集群</h2>
<p>类似Mysql，也是拥有代理连接而非直接连向数据库。使用的是redis cluster，是一种官方的高可用方案。<br>
<img src="https://dxone1.github.io//post-images/1639229473466.png" alt="" loading="lazy"><br>
基于hash槽，通过桶分配类似的操作分配<br>
<img src="https://dxone1.github.io//post-images/1639229560662.png" alt="" loading="lazy"><br>
Redis-cluster有着一些限制，因为节点分开无法支持跨slot的批量操作，同时只支持db0这一个数据库空间。网络路由也可能有更大的开销。按照三主三从的方式测试部署一个Redis集群</p>
<h3 id="部署测试">部署测试</h3>
<ol>
<li>使用如下脚本进行部署</li>
</ol>
<pre><code class="language-java">for port in $(seq 7001 7006); \
do \
mkdir -p /mydata/redis/node-${port}/conf
touch /mydata/redis/node-${port}/conf/redis.conf
cat &lt;&lt; EOF &gt;/mydata/redis/node-${port}/conf/redis.conf
port ${port}
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
cluster-announce-ip 192.168.180.3
cluster-announce-port ${port}
cluster-announce-bus-port 1${port}
appendonly yes
EOF
docker run -p ${port}:${port} -p 1${port}:1${port} --name redis-${port} \
-v /mydata/redis/node-${port}/data:/data \
-v /mydata/redis/node-${port}/conf/redis.conf:/etc/redis/redis.conf \
-d redis:5.0.7 redis-server /etc/redis/redis.conf; \
done
</code></pre>
<ol start="2">
<li>建立集群<br>
进入到一个节点的bash中，使用如下命令建立集群</li>
</ol>
<pre><code class="language-java">redis-cli --cluster create 192.168.180.3:7001 192.168.180.3:7002 192.168.180.3:7003 192.168.180.3:7004 192.168.180.3:7005 192.168.180.3:7006 --cluster-replicas 1
</code></pre>
<p><img src="https://dxone1.github.io//post-images/1639230944281.png" alt="" loading="lazy"><br>
最终集群方式如下<br>
<img src="https://dxone1.github.io//post-images/1639230995066.png" alt="" loading="lazy"><br>
数据的主从和分片存储：<br>
<img src="https://dxone1.github.io//post-images/1639231157661.png" alt="" loading="lazy"><br>
主从保存：<br>
<img src="https://dxone1.github.io//post-images/1639231320662.png" alt="" loading="lazy"><br>
当关闭主节点7001模仿突发宕机，可以看到从节点7004晋升为主节点。<br>
<img src="https://dxone1.github.io//post-images/1639288922997.png" alt="" loading="lazy"><br>
当7001被重新恢复，可以看到它变成了7004的从节点。<br>
<img src="https://dxone1.github.io//post-images/1639289288776.png" alt="" loading="lazy"></p>
<h2 id="es集群">ES集群</h2>
<ul>
<li>一个运行中的 Elasticsearch 实例称为一个节点， 而集群是由一个或者多个拥有相同cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。 当有节点加入集群中或者从集群中移除节点时， 集群将会重新平均分布所有的数据。</li>
<li>当一个节点被选举成为 主节点时， 它将负责管理集群范围内的所有变更， 例如增加、删除索引， 或者增加、 删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作， 所以当集群只拥有一个主节点的情况下， 即使流量的增加它也不会成为瓶颈。任何节点都可以成为主节点。 我们的示例集群就只有一个节点， 所以它同时也成为了主<br>
节点。</li>
<li>作为用户， 我们可以将请求发送到 集群中的任何节点 ， 包括主节点。 每个节点都知道任意文档所处的位置， 并且能够将我们的请求直接转发到存储我们所需文档的节点。无论我们将请求发送到哪个节点， 它都能负责从各个包含我们所需文档的节点收集回数据， 并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。</li>
</ul>
<h3 id="es状态">ES状态</h3>
<p>使用<code>GET /_cluster/health</code>查看集群的状态。有三种状态：green 、 yellow 或者 red。如果设定单机状态下分片为3，复制为1，因为无法复制一份备份而三个分片可以部署，所以状态为yellow。</p>
<pre><code class="language-java">PUT /blogs{
    &quot;settings&quot; : {
        &quot;number_of_shards&quot; : 3,
        &quot;number_of_replicas&quot; : 1
    }
}
</code></pre>
<p><strong>旧版如何解决脑裂问题：</strong><br>
采用类似raft的策略，只有集群中一半以上的节点投票成功才晋升。合并时按照更高选举轮次决定。</p>
<h3 id="构建集群">构建集群</h3>
<p>构建一个三注三从的ES集群测试。模拟三台服务器，每台服务器安装一个data节点和一个master节点。<br>
<img src="https://dxone1.github.io//post-images/1639290424941.png" alt="" loading="lazy"></p>
<ol>
<li>运行命令，防止JVM报错<br>
<code>sysctl -w vm.max_map_count=262144</code></li>
<li>准备一个docker网络，模拟三台服务器<br>
<code>docker network create --driver bridge --subnet=172.18.12.0/16 --gateway=172.18.1.1 mynet</code></li>
<li>使用脚本创建三个master节点和三个data节点</li>
</ol>
<pre><code class="language-java">// 创建master节点
for port in $(seq 1 3); \
do \
mkdir -p /mydata/elasticsearch/master-${port}/config
mkdir -p /mydata/elasticsearch/master-${port}/data
chmod -R 777 /mydata/elasticsearch/master-${port}
cat &lt;&lt; EOF &gt;/mydata/elasticsearch/master-${port}/config/elasticsearch.yml
cluster.name: my-es #集群的名称， 同一个集群该值必须设置成相同的
node.name: es-master-${port} #该节点的名字
node.master: true #该节点有机会成为 master 节点
node.data: false #该节点不可以存储数据
network.host: 0.0.0.0
http.host: 0.0.0.0 #所有 http 均可访问
http.port: 920${port}
transport.tcp.port: 930${port}
#discovery.zen.minimum_master_nodes: 2 #设置这个参数来保证集群中的节点可以知道其它 N 个有 master 资格的节点。 官方推荐（N/2） +1
discovery.zen.ping_timeout: 10s #设置集群中自动发现其他节点时 ping 连接的超时时间
discovery.seed_hosts: [&quot;172.18.12.21:9301&quot;, &quot;172.18.12.22:9302&quot;, &quot;172.18.12.23:9303&quot;] #设置集群中的 Master 节点的初始列表， 可以通过这些节点来自动发现其他新加入集群的节点， es7的新增配置
cluster.initial_master_nodes: [&quot;172.18.12.21&quot;] #新集群初始时的候选主节点， es7 的新增配置
EOF
docker run --name elasticsearch-node-${port} \
-p 920${port}:920${port} -p 930${port}:930${port} \
--network=mynet --ip 172.18.12.2${port} \
-e ES_JAVA_OPTS=&quot;-Xms300m -Xmx300m&quot; \
-v /mydata/elasticsearch/master-${port}/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \
-v /mydata/elasticsearch/master-${port}/data:/usr/share/elasticsearch/data \
-v /mydata/elasticsearch/master-${port}/plugins:/usr/share/elasticsearch/plugins \
-d elasticsearch:7.4.2
done
</code></pre>
<pre><code class="language-java">// 创建data节点
for port in $(seq 4 6); \
do \
mkdir -p /mydata/elasticsearch/node-${port}/config
mkdir -p /mydata/elasticsearch/node-${port}/data
chmod -R 777 /mydata/elasticsearch/node-${port}
cat &lt;&lt; EOF &gt;/mydata/elasticsearch/node-${port}/config/elasticsearch.yml
cluster.name: my-es #集群的名称， 同一个集群该值必须设置成相同的
node.name: es-node-${port} #该节点的名字
node.master: false #该节点有机会成为 master 节点
node.data: true #该节点可以存储数据
network.host: 0.0.0.0
#network.publish_host: 192.168.56.10 #互相通信 ip， 要设置为本机可被外界访问的 ip， 否则无法通信
http.host: 0.0.0.0 #所有 http 均可访问
http.port: 920${port}
transport.tcp.port: 930${port}
#discovery.zen.minimum_master_nodes: 2 #设置这个参数来保证集群中的节点可以知道其它 N 个有 master 资格的节点。 官方推荐（N/2） +1
discovery.zen.ping_timeout: 10s #设置集群中自动发现其他节点时 ping 连接的超时时间
discovery.seed_hosts: [&quot;172.18.12.21:9301&quot;, &quot;172.18.12.22:9302&quot;, &quot;172.18.12.23:9303&quot;] #设置集群中的 Master 节点的初始列表， 可以通过这些节点来自动发现其他新加入集群的节点， es7的新增配置
cluster.initial_master_nodes: [&quot;172.18.12.21&quot;] #新集群初始时的候选主节点， es7 的新增配置
EOF
docker run --name elasticsearch-node-${port} \
-p 920${port}:920${port} -p 930${port}:930${port} \
--network=mynet --ip 172.18.12.2${port} \
-e ES_JAVA_OPTS=&quot;-Xms300m -Xmx300m&quot; \
-v/mydata/elasticsearch/node-${port}/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \
-v /mydata/elasticsearch/node-${port}/data:/usr/share/elasticsearch/data \
-v /mydata/elasticsearch/node-${port}/plugins:/usr/share/elasticsearch/plugins \
-d elasticsearch:7.4.2
done
</code></pre>
<h2 id="rabbitmq集群">RabbitMQ集群</h2>
<p>RabbiMQ 是用 Erlang 开发的， 集群非常方便， 因为 Erlang 天生就是一门分布式语言， 但其本身并不支持负载均衡。RabbitMQ 集群中节点包括内存节点(RAM)、 磁盘节点(Disk， 消息持久化)， 集群中至少有一个 Disk 节点。</p>
<ul>
<li>普通模式(默认)<br>
对于普通模式， 集群中各节点有相同的队列结构， 但消息只会存在于集群中的一个节点。 对于消费者来说， 若消息进入 A 节点的 Queue 中， 当从 B 节点拉取时， RabbitMQ 会将消息从 A 中取出， 并经过 B 发送给消费者。<br>
应用场景： 该模式各适合于消息无需持久化的场合， 如日志队列。 当队列非持久化， 且创建该队列的节点宕机， 客户端才可以重连集群其他节点， 并重新创建队列。 若为持久化，只能等故障节点恢复。</li>
<li>镜像模式<br>
与普通模式不同之处是消息实体会主动在镜像节点间同步， 而不是在取数据时临时拉取， 高可用； 该模式下， mirror queue 有一套选举算法，即 1 个 master、 n 个 slaver， 生产者、 消费者的请求都会转至 master。<br>
应用场景： 可靠性要求较高场合， 如下单、 库存队列。<br>
缺点： 若镜像队列过多， 且消息体量大， 集群内部网络带宽将会被此种同步通讯所消耗。</li>
</ul>
<h3 id="测试">测试</h3>
<ol>
<li>搭建集群</li>
</ol>
<pre><code class="language-java">docker run -d --hostname rabbitmq01 --name rabbitmq01 -v /mydata/rabbitmq/rabbitmq01:/var/lib/rabbitmq -p 15673:15672 -p 5673:5672 -e RABBITMQ_ERLANG_COOKIE='atguigu' rabbitmq:management

docker run -d --hostname rabbitmq02 --name rabbitmq02 -v /mydata/rabbitmq/rabbitmq02:/var/lib/rabbitmq -p 15674:15672 -p 5674:5672 -e RABBITMQ_ERLANG_COOKIE='atguigu' --link rabbitmq01:rabbitmq01 rabbitmq:management

docker run -d --hostname rabbitmq03 --name rabbitmq03 -v /mydata/rabbitmq/rabbitmq03:/var/lib/rabbitmq -p 15675:15672 -p 5675:5672 -e RABBITMQ_ERLANG_COOKIE='atguigu' --link rabbitmq01:rabbitmq01 --link rabbitmq02:rabbitmq02 rabbitmq:management
</code></pre>
<ol start="2">
<li>节点加入集群<br>
<img src="https://dxone1.github.io//post-images/1639297326597.png" alt="" loading="lazy"></li>
<li>建立镜像集群<br>
在master节点使用<code>rabbitmqctl set_policy -p / ha &quot;^&quot; '{&quot;ha-mode&quot;:&quot;all&quot;,&quot;ha-sync-mode&quot;:&quot;automatic&quot;}'</code>设置所有队列为高可用模式。这样就实现了高可用的mq备份。</li>
</ol>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#mysql%E9%9B%86%E7%BE%A4">MySQL集群</a>
<ul>
<li><a href="#mysql%E4%BC%81%E4%B8%9A%E7%BA%A7%E9%83%A8%E7%BD%B2">Mysql企业级部署</a></li>
<li><a href="#shardingsphere">ShardingSphere</a></li>
</ul>
</li>
<li><a href="#redis%E9%9B%86%E7%BE%A4">Redis集群</a>
<ul>
<li><a href="#%E9%83%A8%E7%BD%B2%E6%B5%8B%E8%AF%95">部署测试</a></li>
</ul>
</li>
<li><a href="#es%E9%9B%86%E7%BE%A4">ES集群</a>
<ul>
<li><a href="#es%E7%8A%B6%E6%80%81">ES状态</a></li>
<li><a href="#%E6%9E%84%E5%BB%BA%E9%9B%86%E7%BE%A4">构建集群</a></li>
</ul>
</li>
<li><a href="#rabbitmq%E9%9B%86%E7%BE%A4">RabbitMQ集群</a>
<ul>
<li><a href="#%E6%B5%8B%E8%AF%95">测试</a></li>
</ul>
</li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://dxone1.github.io/post/shang-cheng-xiang-mu-day21kubernetes/">
              <h3 class="post-title">
                商城项目 day21：Kubernetes 
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://dxone1.github.io//atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
